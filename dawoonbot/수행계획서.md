# 여자친구 맞춤형 챗봇 개발 수행 계획서

## 1. 프로젝트 개요

- **목표:** 사용자의 카카오톡 대화 내용을 학습하여 사용자의 말투를 모방하는 챗봇을 개발하고, 여자친구가 웹 브라우저를 통해 쉽게 사용할 수 있도록 배포합니다.
- **주요 제약 조건:** 모델 Fine-tuning을 로컬 컴퓨터(RTX 3060 Ti, 8GB VRAM)에서 수행합니다.
- **데이터 소스:** 사용자-여자친구 간 카카오톡 대화 텍스트 (약 93MB)

## 2. 모델 선정 계획

- **초기 고려 모델:** `microsoft/bitnet-b1.58-2B-4T`
- **분석 결과:**
    - 해당 모델은 8GB VRAM 환경에서 Fine-tuning이 현실적으로 매우 어렵습니다. (BF16 버전 필요, VRAM 요구량 높음)
    - 전용 C++ 구현 외에는 추론 효율성 이점을 얻기 어렵습니다.
    - 한국어 지원이 제한적입니다.
    - PEFT(LoRA, QLoRA 등) 적용의 효과 및 방법이 불확실합니다.
- **대안 모델 전략:**
    - **8GB VRAM + QLoRA 환경에 적합한 한국어 특화 소형 LLM**을 탐색하고 선정합니다.
    - **후보군:**
        - KoAlpaca (EleutherAI/polyglot-ko 기반 소형 버전, 예: 1.3B)
        - EEVE-Korean (EEVE 기반 소형 버전)
        - 기타 Hugging Face Hub의 1B ~ 3B 규모 한국어 모델 (QLoRA 적용 후기 확인 필요)
    - **선정 기준:** 파라미터 크기, QLoRA 적용 시 VRAM 사용량 후기, 한국어 처리 능력 등을 종합적으로 고려합니다.

## 3. 개발 절차

### 3.1. 데이터 준비 및 전처리

- **목표:** 카카오톡 대화 내용을 모델 학습에 적합한 형식으로 변환합니다.
- **세부 절차:**
    1.  **카카오톡 대화 내보내기:** `.txt` 파일로 저장합니다.
    2.  **데이터 파싱:** Python 스크립트 (`re`, `pandas`)를 사용하여 날짜, 시간, 발화자, 메시지 분리.
        - 이모티콘, 사진/동영상 메시지, 링크, 시스템 메시지 처리 방안 결정 (제거 또는 대체).
    3.  **학습 데이터셋 구성:** '나'의 말투 학습을 위해 '여자친구 발화' -> '나의 발화' 쌍으로 구성 (Instruction-Following 형식).
        ```json
        {"instruction": "여자친구가 이렇게 말했어:", "input": "오늘 저녁 뭐 먹을까?", "output": "음... 나는 파스타가 좋은데! 자기는?"}
        ```
    4.  **데이터 정제:** **개인정보(이름, 전화번호, 주소 등) 완전 제거/마스킹 필수.** 불필요하거나 반복적인 대화 제거.
    5.  **데이터 분할:** 학습 데이터와 검증 데이터로 분할 (예: 90:10).
    6.  **데이터 양 고려:** 93MB는 상대적으로 적은 양이므로 과적합 가능성이 높습니다. 데이터 품질 향상 및 핵심 대화 추출이 중요합니다.
- **추천 도구:** Python (`re`, `pandas`), Hugging Face `datasets`

### 3.2. 모델 Fine-tuning (QLoRA 활용)

- **목표:** 준비된 데이터셋으로 선택한 대안 모델을 사용자의 말투에 맞게 Fine-tuning 합니다.
- **세부 절차:**
    1.  **환경 설정:** PyTorch, `transformers`, `peft`, `bitsandbytes`, `accelerate` 등 설치 및 CUDA 설정.
    2.  **모델/토크나이저 로드:** 선택한 대안 모델을 4-bit 양자화(`load_in_4bit=True`)하여 로드 (`bitsandbytes` 활용).
    3.  **QLoRA 설정:** `peft` 라이브러리로 LoRA 설정 (대상 모듈, 랭크 `r`, `lora_alpha`, `lora_dropout` 등).
    4.  **데이터 준비:** `datasets`로 데이터셋 로드 및 토크나이저로 인코딩.
    5.  **Trainer 설정:** `Trainer` 또는 `SFTTrainer` 사용. 학습 파라미터 설정 (학습률, **배치 크기(VRAM 고려하여 작게 설정)**, 에포크, **그래디언트 축적 사용**).
    6.  **학습 실행:** `Trainer.train()` 실행. 검증 손실 모니터링.
    7.  **모델 저장:** 학습된 LoRA 어댑터 가중치 저장.
- **추천 도구:** PyTorch, Hugging Face (`transformers`, `peft`, `bitsandbytes`, `accelerate`, `datasets`)

### 3.3. 웹 기반 챗봇 인터페이스 개발

- **목표:** 여자친구가 쉽게 사용할 수 있는 웹 UI 개발.
- **세부 절차:**
    1.  **프레임워크 선택:** Streamlit 또는 Gradio (Python 기반, UI 개발 용이).
        - **Streamlit:** `st.chat_input`, `st.chat_message` 활용.
        - **Gradio:** `gr.ChatInterface` 활용, HF Spaces 통합 용이.
    2.  **모델 로드:** 원본 베이스 모델 + 학습된 LoRA 어댑터 결합. 추론 시 VRAM 절약을 위해 4-bit 또는 8-bit 양자화 로드 고려.
    3.  **챗봇 로직 구현:** 사용자 입력 처리 -> 대화 기록 관리 -> 모델 `generate()` 호출 (생성 파라미터 조절) -> 응답 디코딩 및 출력.
    4.  **UI 구성:** 채팅 입력창, 대화 내용 표시 영역 등 구성.
- **추천 도구:** Streamlit (`streamlit`), Gradio (`gradio`), PyTorch, `transformers`, `peft`, `bitsandbytes`

### 3.4. 챗봇 배포

- **목표:** 개발된 챗봇 웹 앱 배포 (저렴하고 간편한 방법 우선).
- **세부 절차 (옵션):**
    1.  **Hugging Face Spaces (가장 추천):** 무료 티어, Git 기반, Gradio/Streamlit 직접 호스팅, GPU 옵션(유료).
    2.  **Streamlit Community Cloud:** Streamlit 앱 특화, 무료 티어, GitHub 연동.
    3.  **기타 PaaS (Render 등):** 유연성 높으나 설정 복잡도 증가, 무료 티어 자원 제한 확인 필요.
    4.  **로컬 실행 + ngrok:** 임시 공유용으로 가능.
- **추천 플랫폼:** Hugging Face Spaces, Streamlit Community Cloud

## 4. 예상 어려움 및 주의사항

- **VRAM 제약 (8GB):** QLoRA 사용해도 배치 크기, 시퀀스 길이 제약. OOM 발생 가능성. 최적화 필수.
- **데이터 품질 및 양 (93MB):** 과적합 위험 높음. 일반 대화 능력 한계 예상. 데이터 정제 및 핵심 대화 추출 중요.
- **모델 성능 한계:** 소형 LLM의 본질적인 한계 (어색함, 반복). 현실적인 기대치 설정.
- **한국어 처리:** 모델/토크나이저의 구어체, 신조어, 이모티콘 처리 능력 부족 가능성.
- **개인정보 보호:** **데이터 전처리 시 철저한 익명화 필수.** 배포 시 보안 유의.
- **추론 속도 및 비용:** 모델 크기/양자화/하드웨어 따라 속도 변화. 무료 티어 자원 제한 고려.

## 5. 워크플로우 다이어그램

```mermaid
graph LR
    A[데이터 준비: 카톡 대화(93MB) -> 학습셋] --> B(모델 선정: 한국어 소형 LLM + QLoRA);
    B --> C{Fine-tuning (QLoRA @ 8GB VRAM)};
    C --> D[웹 인터페이스 개발 (Gradio/Streamlit)];
    D --> E{배포 (HF Spaces/Streamlit Cloud)};
    E --> F(여자친구 사용 🎉);

    style A fill:#f9f,stroke:#333,stroke-width:2px
    style B fill:#ccf,stroke:#333,stroke-width:2px
    style C fill:#fcc,stroke:#333,stroke-width:2px
    style D fill:#cfc,stroke:#333,stroke-width:2px
    style E fill:#ffc,stroke:#333,stroke-width:2px
    style F fill:#cff,stroke:#333,stroke-width:2px